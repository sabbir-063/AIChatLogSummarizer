{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc8f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sabbir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sabbir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sabbir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # For sentence tokenization\n",
    "nltk.download('stopwords') # For common stop words\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d046831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to count user, ai, and all messages\n",
    "def parse_info(file_path):\n",
    "    user_messages = []\n",
    "    ai_messages = []\n",
    "    all_messages = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"User:\"):\n",
    "                    message = line[len(\"User:\"):].strip()\n",
    "                    print(f\"User message: {message}\")\n",
    "                    user_messages.append(message)\n",
    "                    all_messages.append(message)\n",
    "                elif line.startswith(\"AI:\"):\n",
    "                    message = line[len(\"AI:\"):].strip()\n",
    "                    print(f\"AI message: {message}\")\n",
    "                    ai_messages.append(message)\n",
    "                    all_messages.append(message)\n",
    "        return user_messages, ai_messages, all_messages\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60375352",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_file = \"chat.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bfdb7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User message: Hello!\n",
      "AI message: Hi! How can I assist you today?\n",
      "User message: Can you explain what machine learning is?\n",
      "AI message: Certainly! Machine learning is a field of AI that allows systems to learn from data.\n",
      "User message: What are some applications of machine learning?\n",
      "AI message: Machine learning is used in recommendation systems, natural language processing, image recognition etc.\n",
      "User message: That sounds interesting. Thanks!\n",
      "AI message: You're welcome!\n"
     ]
    }
   ],
   "source": [
    "user_msg, ai_msg, all_msg = parse_info(chat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc69120",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_exchang, user_msg_cnt, ai_msg_cnt = len(user_msg) + len(ai_msg), len(user_msg), len(ai_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49198e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword extraction with NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def extract_keywords_nltk(messages, top_n=5):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # custom stop words\n",
    "    custom_stop_words = {\n",
    "        'hi', 'hello', 'certainly', 'today', 'can', 'you', 'i', 'more',\n",
    "                         'the', 'a', 'an', 'what', 'some', 'to', 'for', 'that', 'your',\n",
    "                         'of', 'how', 'explain', 'thanks', 'welcome', 'used', 'my', 'its'\n",
    "    }\n",
    "    all_stop_words = stop_words.union(custom_stop_words)\n",
    "\n",
    "    all_words = []\n",
    "    for message in messages:\n",
    "        print(f\"Processing message: {message}\")\n",
    "        # Tokenize and convert to lowercase\n",
    "        tokens = word_tokenize(message.lower(), language='english', preserve_line=True)\n",
    "        # Filter out punctuation and stop words\n",
    "        filtered_words = [\n",
    "            word for word in tokens\n",
    "            if word.isalpha() and word not in all_stop_words\n",
    "        ]\n",
    "        all_words.extend(filtered_words)\n",
    "\n",
    "    word_counts = Counter(all_words)\n",
    "    return [word for word, count in word_counts.most_common(top_n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f19d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt and stopwords downloaded\n"
     ]
    }
   ],
   "source": [
    "#for word_tokenize debugging perpose\n",
    "import os\n",
    "nltk_data_dir = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir, quiet=True)\n",
    "\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "print(\"punkt and stopwords downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d39caa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'Hi! How can I assist you today?', 'Can you explain what machine learning is?', 'Certainly! Machine learning is a field of AI that allows systems to learn from data.', 'What are some applications of machine learning?', 'Machine learning is used in recommendation systems, natural language processing, image recognition etc.', 'That sounds interesting. Thanks!', \"You're welcome!\"]\n",
      "Processing message: Hello!\n",
      "Processing message: Hi! How can I assist you today?\n",
      "Processing message: Can you explain what machine learning is?\n",
      "Processing message: Certainly! Machine learning is a field of AI that allows systems to learn from data.\n",
      "Processing message: What are some applications of machine learning?\n",
      "Processing message: Machine learning is used in recommendation systems, natural language processing, image recognition etc.\n",
      "Processing message: That sounds interesting. Thanks!\n",
      "Processing message: You're welcome!\n"
     ]
    }
   ],
   "source": [
    "print(all_msg)\n",
    "top_keywords = extract_keywords_nltk(all_msg, top_n=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c59faf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine', 'learning', 'systems', 'assist', 'field']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1570a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "The conversation had 8 exchanges.\n",
      "The user asked mainly about machine and learning.\n",
      "Most common keywords: machine, learning, systems, assist, field.\n"
     ]
    }
   ],
   "source": [
    "#printing summary\n",
    "print(\"Summary:\")\n",
    "print(f\"The conversation had {total_exchang} exchanges.\")\n",
    "# print(f\"User messages: {user_msg_cnt}\")\n",
    "# print(f\"AI messages: {ai_msg_cnt}\")\n",
    "\n",
    "if top_keywords:\n",
    "    if len(top_keywords) >= 2:\n",
    "        print(f\"The user asked mainly about {top_keywords[0]} and {top_keywords[1]}.\")\n",
    "    elif len(top_keywords) == 1:\n",
    "        print(f\"The user asked mainly about {top_keywords[0]}.\")\n",
    "    else:\n",
    "        print(\"The user asked mainly about general topics.\")\n",
    "    \n",
    "    print(f\"Most common keywords: {', '.join(top_keywords)}.\")\n",
    "else:\n",
    "    print(\"No significant keywords found after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ee9c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done and dusted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
